\documentclass[12pt]{article}

\input preamble

\title{Principles of Parallel Architecture\\
Project Report 2}
\author{Xitong Liu \\
xliu@ece.udel.edu}

\begin{document}

\maketitle

\section{Introduction}
In this report, we will introduce the methods applied to optimize
the performance of Matrix Multiplication on parallel architecture.
Results showed that the performance has been improved a lot comparing
to the baseline approach proposed in the previous report.

\section{Baseline}
The setup of the baseline parallel is straightforward, as we divide
one matrix operand evenly into several parts, each part is distributed
to one thread. The approach is shown in Figure~\ref{fig:matrix-divide}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{matrix-divide.png}
		\caption{\label{fig:matrix-divide}Matrix Division}
	\end{center}
\end{figure}

\section{Cache Line}
In the baseline version, for $\mathbf{C}=\mathbf{A}\times\mathbf{B}$,
in the inner most loop, $\mathbf{A}$ is accessed in a row major. When 
the first element of one row in $\mathbf{A}$ is accessed, the access 
to the following elements can benefit from the cache line since they
are adjacent in memory and loaded in the same cacheline. However, 
$\mathbf{B}$ is accessed in a column major and the access of the 
following elements may cause more loads of cache line. The sample code 
for $\mathbf{C}=\mathbf{A}\times\mathbf{B}$ is shown as below:

\begin{verbatim}
DO I = 1, N 
  DO J = 1, N
	  C(I,J) = 0.0 
      DO K = 1, N
      C(I,J) = C(I,J) + A(I,K) * B(K,J) 
    ENDDO
  ENDDO
ENDDO
\end{verbatim}

We transpose $\mathbf{B}$ to $\mathbf{B^{T}}$ at first. In the 
following multiplication operation's inner most loop, access of 
both $\mathbf{A}$ and $\mathbf{B^{T}}$ is in row major and both
of them can benefit from the acceleration of cache line. More 
detail is depicted in Figure~\ref{fig:cacheline}. The sample code 
is shown as below:

\begin{verbatim}
DO I = 1, N 
  DO J = 1, N
	  C(I,J) = 0.0 
      DO K = 1, N
      C(I,J) = C(I,J) + A(I,K) * BT(J,K) 
    ENDDO
  ENDDO
ENDDO
\end{verbatim}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{cacheline.png}
		\caption{\label{fig:cacheline}Matrix Transpose}
	\end{center}
\end{figure}

\section{SSE}
Streaming SIMD Extension (SSE) is a SIMD instruction set 
introduced by Intel, begin in Pentium III product line. 
\footnote{http://en.wikipedia.org/wiki/Streaming\_SIMD\_Extensions}
It adds eight new 128-bit registers to the CPU, known as XMM0 
through XMM7. On Intel 64 platform, in which the experiment is 
conducted, the registered have been extended from XMM0 to XMM15.
Each register can hold 128 bit data, e.g. four 32-bit single-precision
floating point numbers, or two 64-bit double-precision floating point
numbers. Specific floating operation instructions can be issued in a
separate FPU and they can operation on the whole 128-bit register.
With the help of SSE, the floating operation instructions can be
executed in parallel with other instructions which may lead great 
performance boost in pipelining. 

GCC has built-in support for SSE, 
\footnote{http://gcc.gnu.org/onlinedocs/gcc-4.4.1/gcc/X86-Built\_002din-Functions.html} 
and all the SSE instructions are encapsulated in built-in functions with 
prefix \texttt{\_\_builtin\_ia32}. The code snippet is shown as below:

\footnotesize
\begin{verbatim}
// SSE
typedef float v4sf __attribute__ ((vector_size(16)));
#define SSE_LENGTH 4

v4sf acc;
v4sf oprand_a;
v4sf oprand_b;
ftype imd_ret[SSE_LENGTH];

for(cycleI = start; cycleI < end; ++ cycleI){
  for(cycleJ = 0; cycleJ < matrixB.xDim; ++ cycleJ){
  // set acc to 0
  acc = __builtin_ia32_loadups(init_array);
        
  for(cycleK = 0; cycleK < (dim - SSE_LENGTH + 1); cycleK += SSE_LENGTH){
    oprand_a = __builtin_ia32_loadups(&(matrixA.data[cycleI * dim + cycleK]));
    oprand_b = __builtin_ia32_loadups(&(matrixBT.data[cycleJ * dim + cycleK]));
    acc = __builtin_ia32_addps(acc, __builtin_ia32_mulps(oprand_a, oprand_b));
  }
  __builtin_ia32_storeups(imd_ret, acc);

  for(cycleK = 1; cycleK < SSE_LENGTH; ++ cycleK){
    imd_ret[0] += imd_ret[cycleK];
  }
  matrixC.data[cycleI * dim + cycleJ] = imd_ret[0];
}
\end{verbatim}
\normalsize

\texttt{v4sf} is defined as vector type which is composed of 4 single-precision
floating point numbers and it is stored in XMM registers. 
\texttt{\_\_builtin\_ia32\_loadups} is used to load data from main memory to 
XMM registers, and \texttt{\_\_builtin\_ia32\_storeups} is used to store the 
data from XMM registers to main memory. \texttt{\_\_builtin\_ia32\_addps} and 
\texttt{\_\_builtin\_ia32\_mulps} are used to do XMM add and multiply operation 
respectively. Since each operation can manipulate 4 float numbers a time, the 
stride of the inner most loop is set to 4.

\section{Unrolling and Scheduling}
To better utilize the benefits of the pipelining, we apply unrolling and scheduling.
Considering the limitation that there are 16 XMM registers per core, we unroll the
inner loop by 4 steps and reschedule the execution order to hide the load and
store latency. The code snippet is shown as below:

\footnotesize
\begin{verbatim}
for(cycleK = 0; cycleK < (dim - SSE_LENGTH + 1); cycleK += SSE_LENGTH){
  oprand_a = __builtin_ia32_loadups(&(matrixA.data[cycleI * dim + cycleK]));

  oprand_b_0 = __builtin_ia32_loadups(&(matrixBT.data[cycleJ * dim + cycleK]));
  oprand_b_1 = __builtin_ia32_loadups(&(matrixBT.data[(cycleJ + 1) * dim + cycleK]));
  oprand_b_2 = __builtin_ia32_loadups(&(matrixBT.data[(cycleJ + 2) * dim + cycleK]));
  oprand_b_3 = __builtin_ia32_loadups( &(matrixBT.data[(cycleJ + 3) * dim + cycleK]));
          
  acc_0 = __builtin_ia32_addps(acc_0, __builtin_ia32_mulps(oprand_a, oprand_b_0));
  acc_1 = __builtin_ia32_addps(acc_1, __builtin_ia32_mulps(oprand_a, oprand_b_1));
  acc_2 = __builtin_ia32_addps(acc_2, __builtin_ia32_mulps(oprand_a, oprand_b_2));
  acc_3 = __builtin_ia32_addps(acc_3, __builtin_ia32_mulps(oprand_a, oprand_b_3));
}
\end{verbatim}
\normalsize

\section{Software Pipelining}
We also tried software pipelining to pursue further performance improvement. 
The approach is intuitive, as shown as below:
\footnotesize
\begin{verbatim}
int is_odd = 0;

for(cycleI = start; cycleI < end; ++ cycleI){
  for(cycleJ = 0; cycleJ < matrixB.xDim - 1; cycleJ += 2){
    for(cycleK = SSE_LENGTH; cycleK < (dim - SSE_LENGTH + 1); 
      cycleK += SSE_LENGTH){
      is_odd = cycleK / 4;
      is_odd = is_odd % 2;
      if(is_odd){
        prefetch data of group 1 for the next run;
        do accumulation on group 0;
      }else{
        prefetch data of group 0 for the next run;
        do accumulation on group 1;
      }
    }   
    // calculate the sum for the last run
    if(!is_odd){
    	  do accumulation on group 0;
    }else{
      do accumulation on group 1;
    }
    store the result to matrixC;
  }
}
\end{verbatim}
\normalsize

We prepared 2 group of registers. In the loop iteration, we load data 
into one group of registers and do calculation on the other group so 
that the data loading latency can be hidden by pipelining.

\section{Experiment Results}
\subsection{Experiment Setup}
The matrix size is $1024\times 1024$ in float numbers and number of 
threads is 32. We apply different optimization methods step by step, 
and new optimization method is combined with the previous one. Since 
it's limited by the space, we assign different method combinations to 
different IDs, shown in Table~\ref{tab:methods}.

\begin{table}[h!]
	\small
	\begin{center}
	\caption{\label{tab:methods} Method Combinations ID Map}
	\begin{tabular}{|r|l|}
		\hline
		ID & Method Combination \\ \hline
		0 &	Serial \\ \hline
		1 & Baseline \\ \hline
    2 & Transpose \\ \hline
    3 & Transpose + SSE \\ \hline
    4 & Transpose + SSE + Unrolling (4 cycles) + Scheduling \\ \hline
    5 & Transpose + SSE + Unrolling (2 cycles) + Scheduling + 
      Software pipelining \\ \hline
	\end{tabular}
	\end{center}
\end{table}

\subsection{Results}
To explore the performances under different optimization settings by 
the compiler, we run the experiments in two groups: one with \texttt{-O3}
option and the other without any optimization. For each setup, we 
run the program 20 times and calculate the average run time as the 
measurement of performance. \footnote{Currently we have run into some
problem about applying PAPI with OpenMP, as PAPI can not work correctly
under OpenMP. We will try to fix it and use PAPI to measure the 
performance with more details in the next report.} The results are shown
in Table~\ref{tab:performance}.

\begin{table}[h!]
	\small
	\begin{center}
	\caption{\label{tab:performance} Entity Recall for each topic query 
	based on Entity Name}
	\begin{tabular}{|r|r|r|r|r|}
		\hline
		 & \multicolumn{2}{c|}{NO Optimization} & \multicolumn{2}{c|}{-O3 Optimization} \\ \hline
		Method & Time(s) & Speed Up & Time(s) & Speed Up \\ \hline
		0  &  33.7776	& 1.0000		&19.4682		&	1.0000 \\ \hline
		1  &		4.5976		&	7.3468		& 2.3256		&	8.3712 \\ \hline
		2  &		4.1109  & 8.2166		&	0.4622		&	42.1209 \\ \hline
		3  &		0.9353  & 36.1142	&	0.4656		&	41.8133 \\ \hline
		4  &		0.6091  & \textbf{55.4550}	& 0.4208		&	\textbf {46.2649} \\ \hline
		5  &		0.7761  & 43.5223	&	0.4660		&	41.7774 \\ \hline
	\end{tabular}
	\end{center}
\end{table}

\subsection{Analysis}
Based on the observations in the results, we can draw the following interesting 
conclusions:
\begin{enumerate}
\item Transpose can improve the performance, but not too much;
\item SSE can boost the performance greatly, about $4\times$ faster.
\item Unrolling and Scheduling can improve the performance further, about 50\% faster;
\item Software pipelining can not improve the performance in the current setup. 
Comparing the setup of method 4 and method 5, we can find that this is mainly due to
the limitation of available XMM registers. In method 4, we can unroll 4 cycles, while
in method 5 we can unroll 2 cycles because we need two groups of registers to 
interweave load and calculation operations. This is the tradeoff of unrolling and
software pipelining.
\item When -O3 option is turned on, there differences between methods with SSE 
applied are very small, since the complier has done a lot of work to do unrolling,
scheduling and software pipelining, etc.
\item Our current best performance (Method 4) without any optimization has reached 
the 60\% of the best performance with -O3 optimization.
\end{enumerate}

\section{Conclusion}
Currently we have tried four different optimization methods in this experiment, 
and results showed that our performance has been improved a lot and reach the 
near limit of the ideal best performance. The best setup can run $55\times$ 
faster that the serial version. Also, the performance of best setup is
approaching the ideal limit.

In the following days, we will apply some other methods to do the final try
to reach the optimization peak.

\end{document}

\begin{comment}
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.7\textwidth, angle=0]{fatest.png}
		\caption{\label{fig:fatest}Fatest SuperComputer in the world}
	\end{center}
\end{figure}
\end{comment}